{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac633e1",
   "metadata": {},
   "source": [
    "# 스케줄 간격"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad5087b",
   "metadata": {},
   "source": [
    "- Airflow에서 DAG 일정 시간 간격으로 실행하기\n",
    "- 시간, 일, 월 등\n",
    "- schedule_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d242190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하루에 한 번 DAG 실행하기\n",
    "dag = DAG(\n",
    "    dag_id=\"listing_2_10\",\n",
    "    start_date=airflow.utils.dates.days_ago(14), # 14일 전부터 시작 \n",
    "    schedule_interval=\"@daily\", # 워크플로 하루에 한 번 실행\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0288a5d",
   "metadata": {},
   "source": [
    "# Airflow 태스크 실패"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc4602",
   "metadata": {},
   "source": [
    "- 외부 서비스 중단, 네트워크 연결 문제, 디스크 손상 등\n",
    "- 로그를 열어서 스택 확인 후 잠재적 문제 원인 확인\n",
    "- 문제 해결 후 실패한 시점부터 다시 태스크 시작(전체 워크프로우 재실행 X)\n",
    "- 실패한 태스크 클릭 후 팝업에서 'Clear' 버튼 클릭"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360b6c0",
   "metadata": {},
   "source": [
    "# 스케줄링 사용자 이벤트 처리 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bbfff4",
   "metadata": {},
   "source": [
    "- 웹사이트에서 사용자 동작 추적 및 웹사이트에서 액세스한 페이지 분석 서비스 가정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1b637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API(로컬) 생성\n",
    "# 30일 동안 모든 이벤트 목록 반환\n",
    "# 사용자 통계를 계산하기 위해 분석할 수 있는 사용자 이벤트 목록 반환\n",
    "\"curl -o /data/events.json http://events_api:5000/events\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eff146",
   "metadata": {},
   "source": [
    "## unscheduled DAG\n",
    "- BashOperator : 사용자 이벤트 데이터\n",
    "- PythonOperator : 데이터 로드 및 이벤트 개수 확인\n",
    "- UI, API 통해서 수동으로 트리거해서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df61403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"01_unscheduled\", \n",
    "    start_date=datetime(2019, 1, 1), # DAG 시작 날짜\n",
    "    schedule_interval=None # 스케줄 X\n",
    ")\n",
    "\n",
    "fetch_events = BashOperator(\n",
    "    task_id=\"fetch_events\",\n",
    "    bash_command=(\n",
    "        \"mkdir -p /data/events && \"\n",
    "        \"curl -o /data/events.json http://events_api:5000/events\" # API에서 이벤트 가져온 후 저장 \n",
    "    ),\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def _calculate_stats(input_path, output_path):\n",
    "    \"\"\"Calculates event statistics.\"\"\"\n",
    "\n",
    "    Path(output_path).parent.mkdir(exist_ok=True) # 출력 디렉터리 확인 \n",
    "\n",
    "    events = pd.read_json(input_path) # 이벤트 데이터 로드 \n",
    "    stats = events.groupby([\"date\", \"user\"]).size().reset_index() # 필요 통계 계산\n",
    "\n",
    "    stats.to_csv(output_path, index=False) #  csv 파일 저장\n",
    "\n",
    "\n",
    "calculate_stats = PythonOperator(\n",
    "    task_id=\"calculate_stats\",\n",
    "    python_callable=_calculate_stats,\n",
    "    op_kwargs={\"input_path\": \"/data/events.json\", \"output_path\": \"/data/stats.csv\"},\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "fetch_events >> calculate_stats # 실행 순서 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36636d72",
   "metadata": {},
   "source": [
    "## daily_schedule DAG\n",
    "- 매일 자정에 DAG 실행\n",
    "- 정의된 간격 후에 태스크 시작\n",
    "- 처음 실행 시간은 1월 2일. 1월 1일에는 실행되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"02_daily_schedule\",\n",
    "    schedule_interval=\"@daily\", # 매일 자정에 실행 \n",
    "    start_date=datetime(2019, 1, 1), # 스케줄 시작 날짜/시간\n",
    "    end_date=datetime(2019, 1, 5),\n",
    ")\n",
    "\n",
    "fetch_events = BashOperator(\n",
    "    task_id=\"fetch_events\",\n",
    "    bash_command=(\n",
    "        \"mkdir -p /data/events && \"\n",
    "        \"curl -o /data/events.json http://events_api:5000/events\"\n",
    "    ),\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def _calculate_stats(input_path, output_path):\n",
    "    \"\"\"Calculates event statistics.\"\"\"\n",
    "\n",
    "    events = pd.read_json(input_path)\n",
    "    stats = events.groupby([\"date\", \"user\"]).size().reset_index()\n",
    "\n",
    "    Path(output_path).parent.mkdir(exist_ok=True)\n",
    "    stats.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "calculate_stats = PythonOperator(\n",
    "    task_id=\"calculate_stats\",\n",
    "    python_callable=_calculate_stats,\n",
    "    op_kwargs={\"input_path\": \"/data/events.json\", \"output_path\": \"/data/stats.csv\"},\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "fetch_events >> calculate_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b9205",
   "metadata": {},
   "source": [
    "## with_end_date DAG\n",
    "- DAG 종료 날짜 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253693b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"03_with_end_date\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=dt.datetime(year=2019, month=1, day=1),\n",
    "    end_date=dt.datetime(year=2019, month=1, day=5),\n",
    ")\n",
    "\n",
    "fetch_events = BashOperator(\n",
    "    task_id=\"fetch_events\",\n",
    "    bash_command=(\n",
    "        \"mkdir -p /data/events && \"\n",
    "        \"curl -o /data/events.json http://events_api:5000/events\"\n",
    "    ),\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def _calculate_stats(input_path, output_path):\n",
    "    \"\"\"Calculates event statistics.\"\"\"\n",
    "\n",
    "    events = pd.read_json(input_path)\n",
    "    stats = events.groupby([\"date\", \"user\"]).size().reset_index()\n",
    "\n",
    "    Path(output_path).parent.mkdir(exist_ok=True)\n",
    "    stats.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "calculate_stats = PythonOperator(\n",
    "    task_id=\"calculate_stats\",\n",
    "    python_callable=_calculate_stats,\n",
    "    op_kwargs={\"input_path\": \"/data/events.json\", \"output_path\": \"/data/stats.csv\"},\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "fetch_events >> calculate_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee321e53",
   "metadata": {},
   "source": [
    "## Cron 기반의 스케줄 간격 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6662c05",
   "metadata": {},
   "source": [
    "- cron : macOS, 리눅스 등과 같은 유닉스 기반 OS에서 사용하는 시간 기반 작업 스케줄러\n",
    "- \"\"* * * * *\"\"\n",
    "- 분 시간 일 월 요일 \n",
    "- 0 * * * * : 매시간 정시에 실행\n",
    "- 0 0 * * * : 매일 자정에 실행\n",
    "- 0 0 * * 0 : 매주 일요일 자정에 실행\n",
    "- 0 0 1 * * : 매월 1일 자정\n",
    "- 45 23 * * SAT : 매주 토요일 23시 45분\n",
    "- 0 0 * * MON, WED, FRI : 매주 월, 화, 금요일 자정에 실행\n",
    "- 0 0 * * MON-FRI : 매주 월요일부터 금요일 자정에 실행\n",
    "- 0 0,12 * * * : 매일 자정 및 오후 12시에 실행\n",
    "- 특정 빈도마다 스케줄 정의 불가(ex. 3일에 한 번씩)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2807cfc",
   "metadata": {},
   "source": [
    "## 빈도 기반의 스케줄 간격"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ea4cbd",
   "metadata": {},
   "source": [
    "- dateitme 모듈에 포함된 timedelta 인스턴스 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ebddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"04_time_delta\",\n",
    "    schedule_interval=dt.timedelta(days=3), # 시작 시간으로부터 3일마다 실행\n",
    "    start_date=dt.datetime(year=2019, month=1, day=1),\n",
    "    end_date=dt.datetime(year=2019, month=1, day=5),\n",
    ")\n",
    "\n",
    "fetch_events = BashOperator(\n",
    "    task_id=\"fetch_events\",\n",
    "    bash_command=(\n",
    "        \"mkdir -p /data/events && \"\n",
    "        \"curl -o /data/events.json http://events_api:5000/events\"\n",
    "    ),\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def _calculate_stats(input_path, output_path):\n",
    "    \"\"\"Calculates event statistics.\"\"\"\n",
    "\n",
    "    events = pd.read_json(input_path)\n",
    "    stats = events.groupby([\"date\", \"user\"]).size().reset_index()\n",
    "\n",
    "    Path(output_path).parent.mkdir(exist_ok=True)\n",
    "    stats.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "calculate_stats = PythonOperator(\n",
    "    task_id=\"calculate_stats\",\n",
    "    python_callable=_calculate_stats,\n",
    "    op_kwargs={\"input_path\": \"/data/events.json\", \"output_path\": \"/data/stats.csv\"},\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "fetch_events >> calculate_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73834b",
   "metadata": {},
   "source": [
    "# 데이터 증분 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1eef4e",
   "metadata": {},
   "source": [
    "- 데이터 순차적으로 가져올 수 있도록 DAG 변경\n",
    "- 스케줄 간격에 해당하는 일자의 이벤트만 로드하고 새로운 이벤트만 통계 계산\n",
    "- 데이터의 양 크게 줄여 훨씬 효율적\n",
    "- 날짜별로 분리된 단일 파일로 저장해 매일 순차적으로 파일 저장 가능\n",
    "\n",
    "- 워크플로에서 증분 데이터 처리를 구현하려면 DAG 수정하여 특정 날짜의 데이터 다운로드\n",
    "- 시작 및 종료 날짜 매개변수 함께 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5431e",
   "metadata": {},
   "source": [
    "## query_with_dates DAG\n",
    "- 이벤트 데이터의 시간 범위 매개변수\n",
    "- end date는 포함하지 않는 날짜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"05_query_with_dates\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=dt.datetime(year=2019, month=1, day=1),\n",
    "    end_date=dt.datetime(year=2019, month=1, day=5),\n",
    ")\n",
    "\n",
    "fetch_events = BashOperator(\n",
    "    task_id=\"fetch_events\",\n",
    "    bash_command=(\n",
    "        \"mkdir -p /data/events && \"\n",
    "        \"curl -o /data/events.json \"\n",
    "        \"http://events_api:5000/events?\"\n",
    "        \"start_date=2019-01-01&\"\n",
    "        \"end_date=2019-01-02\"\n",
    "    ),\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def _calculate_stats(input_path, output_path):\n",
    "    \"\"\"Calculates event statistics.\"\"\"\n",
    "\n",
    "    events = pd.read_json(input_path)\n",
    "    stats = events.groupby([\"date\", \"user\"]).size().reset_index()\n",
    "\n",
    "    Path(output_path).parent.mkdir(exist_ok=True)\n",
    "    stats.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "calculate_stats = PythonOperator(\n",
    "    task_id=\"calculate_stats\",\n",
    "    python_callable=_calculate_stats,\n",
    "    op_kwargs={\"input_path\": \"/data/events.json\", \"output_path\": \"/data/stats.csv\"},\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "fetch_events >> calculate_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6e04e",
   "metadata": {},
   "source": [
    "## templated_query DAG\n",
    "- 2019-01-01 이외의 날짜에 대한 데이터 가져오기\n",
    "- 실행 날짜 사용\n",
    "- \" : DAG가 실행되는 날짜와 시간을 나타내는 매개변수(스케줄 간격으로 실행되는 시작 시간)\n",
    "- next_execution_date : 스케줄 간격의 종료 시간\n",
    "- previous_execution : 과거의 스케줄 간격의 시작 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"06_templated_query\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=dt.datetime(year=2019, month=1, day=1),\n",
    "    end_date=dt.datetime(year=2019, month=1, day=5),\n",
    ")\n",
    "\n",
    "fetch_events = BashOperator(\n",
    "    task_id=\"fetch_events\",\n",
    "    bash_command=(\n",
    "        \"mkdir -p /data/events && \"\n",
    "        \"curl -o /data/events.json \"\n",
    "        \"http://events_api:5000/events?\"\n",
    "        \"start_date={{execution_date.strftime('%Y-%m-%d')}}&\" # Jinja 템플릿으로 형식화된 execution_date 삽입\n",
    "        \"end_date={{next_execution_date.strftime('%Y-%m-%d')}}\" # next_execution_date로 다음 실행 간격 날짜 정의\n",
    "    ),\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def _calculate_stats(input_path, output_path):\n",
    "    \"\"\"Calculates event statistics.\"\"\"\n",
    "\n",
    "    events = pd.read_json(input_path)\n",
    "    stats = events.groupby([\"date\", \"user\"]).size().reset_index()\n",
    "\n",
    "    Path(output_path).parent.mkdir(exist_ok=True)\n",
    "    stats.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "calculate_stats = PythonOperator(\n",
    "    task_id=\"calculate_stats\",\n",
    "    python_callable=_calculate_stats,\n",
    "    op_kwargs={\"input_path\": \"/data/events.json\", \"output_path\": \"/data/stats.csv\"},\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "fetch_events >> calculate_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b9676",
   "metadata": {},
   "source": [
    "## templated_query_ds DAG\n",
    "- 축약 매개변수 제공\n",
    "- ds 및 ds_nodash 매개 변수는 각각 YYYY-MM-DD 및 YYYYMMDD 형식으로 된 execution_date의 다른 표현\n",
    "- next_ds, next_ds_nodash\n",
    "- prev_ds, prev_ds_nodash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60755da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"07_templated_query_ds\",\n",
    "    schedule_interval=timedelta(days=3),\n",
    "    start_date=dt.datetime(year=2019, month=1, day=1),\n",
    "    end_date=dt.datetime(year=2019, month=1, day=5),\n",
    ")\n",
    "\n",
    "fetch_events = BashOperator(\n",
    "    task_id=\"fetch_events\",\n",
    "    bash_command=(\n",
    "        \"mkdir -p /data/events && \"\n",
    "        \"curl -o /data/events.json \"\n",
    "        \"http://events_api:5000/events?\"\n",
    "        \"start_date={{ds}}&\" # YYYY-MM_DD 형식의 execution_date 제공 \n",
    "        \"end_date={{next_ds}}\"\n",
    "    ),\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def _calculate_stats(input_path, output_path):\n",
    "    \"\"\"Calculates event statistics.\"\"\"\n",
    "\n",
    "    events = pd.read_json(input_path)\n",
    "    stats = events.groupby([\"date\", \"user\"]).size().reset_index()\n",
    "\n",
    "    Path(output_path).parent.mkdir(exist_ok=True)\n",
    "    stats.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "calculate_stats = PythonOperator(\n",
    "    task_id=\"calculate_stats\",\n",
    "    python_callable=_calculate_stats,\n",
    "    op_kwargs={\"input_path\": \"/data/events.json\", \"output_path\": \"/data/stats.csv\"},\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "fetch_events >> calculate_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef255a3",
   "metadata": {},
   "source": [
    "## templated_path DAG\n",
    "- 데이터 파티셔닝\n",
    "- 파티셔닝 : 데이터 세트를 더 작고 관리하기 쉬운 조각으로 나누는 작업 \n",
    "- 파티션 : 데이터 세트의 작은 부분\n",
    "- 새로운 fetch_events 태스크로 이벤트 데이터 새롭게 스케줄한 간격에 맞춰 점진적으로 가져올 때, 각각의 새로운 태스크가 전일의 데이터 덮어쓰지 않게 함 \n",
    "- events.json 파일에 새 이벤트 추가할 수 있으나 전체 데이터 세트 로드하는 다운스트림 프로세스 작업 필요\n",
    "- 태스크의 출력을 해당 실행 날짜 이름이 적힌 파일에 기록해 데이터 세트 일일 배치로 나누는 방식 사용\n",
    "- 매일 사용자 이벤트에 대한 통계 계산시 전체 데이터 세트 로드 및 전체 이벤트 기록에 대한 통계 계산할 필요 없음\n",
    "- 각 파티션에 대한 통계 효율적으로 계산 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"08_templated_path\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=dt.datetime(year=2019, month=1, day=1),\n",
    "    end_date=dt.datetime(year=2019, month=1, day=5),\n",
    ")\n",
    "\n",
    "fetch_events = BashOperator(\n",
    "    task_id=\"fetch_events\",\n",
    "    bash_command=(\n",
    "        \"mkdir -p /data/events && \"\n",
    "        \"curl -o /data/events/{{ds}}.json \" # 반환된 값이 템플릿 파일 이름에 기록\n",
    "        \"http://events_api:5000/events?\"\n",
    "        \"start_date={{ds}}&\"\n",
    "        \"end_date={{next_ds}}\"\n",
    "    ),\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def _calculate_stats(**context): # 모든 콘텍스트 변수 수신 \n",
    "    \"\"\"Calculates event statistics.\"\"\"\n",
    "    input_path = context[\"templates_dict\"][\"input_path\"] # templates_dict 개체에서 템플릿 값 검색\n",
    "    output_path = context[\"templates_dict\"][\"output_path\"]\n",
    "\n",
    "    events = pd.read_json(input_path)\n",
    "    stats = events.groupby([\"date\", \"user\"]).size().reset_index()\n",
    "\n",
    "    Path(output_path).parent.mkdir(exist_ok=True)\n",
    "    stats.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "calculate_stats = PythonOperator(\n",
    "    task_id=\"calculate_stats\",\n",
    "    python_callable=_calculate_stats,\n",
    "    templates_dict={ # 매개변수 사용. 템플릿화해야 하는 모든 인수 전달\n",
    "        \"input_path\": \"/data/events/{{ds}}.json\",\n",
    "        \"output_path\": \"/data/stats/{{ds}}.csv\",\n",
    "    },\n",
    "    # Required in Airflow 1.10 to access templates_dict, deprecated in Airflow 2+.\n",
    "    # provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "fetch_events >> calculate_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b772b7eb",
   "metadata": {},
   "source": [
    "## no_catchup DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37ec44",
   "metadata": {},
   "source": [
    "- 시작 날짜, 스케줄 간격 및 종료 날짜의 세 가지 매개변수 사용하여 DAG 실행 시점 제어\n",
    "- 임의의 시작 날짜로부터 스케줄 간격 정의 가능\n",
    "- 과거의 시작 날짜부터 과거 간격 정의 가능\n",
    "- Backfilling : 과거 데이터 세트 로드하거나 분석하기 위해 DAG의 과거 시점 지정 실행 \n",
    "- 과거 시작 날짜를 지정하고 해당 DAG 활성화하면 현재 시간 이전에 과거 시작 이후의 모든 스케줄 간격 생성됨\n",
    "- catchup 변수에 의해 제어되며 false로 설정하여 비활성 가능\n",
    "- 가장 최근 스케줄 간격에 대해서만 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"09_no_catchup\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=dt.datetime(year=2019, month=1, day=1),\n",
    "    end_date=dt.datetime(year=2019, month=1, day=5),\n",
    "    catchup=False,\n",
    ")\n",
    "\n",
    "fetch_events = BashOperator(\n",
    "    task_id=\"fetch_events\",\n",
    "    bash_command=(\n",
    "        \"mkdir -p /data/events && \"\n",
    "        \"curl -o /data/events/{{ds}}.json \"\n",
    "        \"http://events_api:5000/events?\"\n",
    "        \"start_date={{ds}}&\"\n",
    "        \"end_date={{next_ds}}\"\n",
    "    ),\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def _calculate_stats(**context):\n",
    "    \"\"\"Calculates event statistics.\"\"\"\n",
    "    input_path = context[\"templates_dict\"][\"input_path\"]\n",
    "    output_path = context[\"templates_dict\"][\"output_path\"]\n",
    "\n",
    "    events = pd.read_json(input_path)\n",
    "    stats = events.groupby([\"date\", \"user\"]).size().reset_index()\n",
    "\n",
    "    Path(output_path).parent.mkdir(exist_ok=True)\n",
    "    stats.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "calculate_stats = PythonOperator(\n",
    "    task_id=\"calculate_stats\",\n",
    "    python_callable=_calculate_stats,\n",
    "    templates_dict={\n",
    "        \"input_path\": \"/data/events/{{ds}}.json\",\n",
    "        \"output_path\": \"/data/stats/{{ds}}.csv\",\n",
    "    },\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "fetch_events >> calculate_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f58dc",
   "metadata": {},
   "source": [
    "## non_atomic_send DAG\n",
    "- 원자성(atomicity) : 모든 것이 완료되거나 완료되지 않아야 함. 나눌 수 없고 돌이킬 수 없는 일련의 데이터베이스와 같은 작업 \n",
    "- 각 실행이 끝날 때마다 상위 10명의 사용자에게 이메일 발송 기능 추가\n",
    "- 통계 계산 함수에 이메일 보내는 함수 추가\n",
    "- _email_stats 함수 실패 시 통계 발송이 실패했음에도 불구하고 작업 성공한 것처럼 보임\n",
    "\n",
    "\n",
    "- 멱등성(dempotency) : 동일한 입력으로 동일한 태스크를 여러 번 호출해도 결과에 효력이 없어야 함\n",
    "- 입력 변경 없이 태스크 다시 실행해도 전체 결과 변경되지 않아야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"10_non_atomic_send\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=dt.datetime(year=2019, month=1, day=1),\n",
    "    end_date=dt.datetime(year=2019, month=1, day=5),\n",
    "    catchup=True,\n",
    ")\n",
    "\n",
    "fetch_events = BashOperator(\n",
    "    task_id=\"fetch_events\",\n",
    "    bash_command=(\n",
    "        \"mkdir -p /data/events && \"\n",
    "        \"curl -o /data/events/{{ds}}.json \"\n",
    "        \"http://events_api:5000/events?\"\n",
    "        \"start_date={{ds}}&\"\n",
    "        \"end_date={{next_ds}}\"\n",
    "    ),\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def _calculate_stats(**context):\n",
    "    \"\"\"Calculates event statistics.\"\"\"\n",
    "    input_path = context[\"templates_dict\"][\"input_path\"]\n",
    "    output_path = context[\"templates_dict\"][\"output_path\"]\n",
    "\n",
    "    events = pd.read_json(input_path)\n",
    "    stats = events.groupby([\"date\", \"user\"]).size().reset_index()\n",
    "\n",
    "    Path(output_path).parent.mkdir(exist_ok=True)\n",
    "    stats.to_csv(output_path, index=False)\n",
    "\n",
    "    _email_stats(stats, email=\"user@example.com\") # csv에 작성 후 이메일 보내면 단일 기능에서 두 가지 작업 수행되어 원자성 깨짐\n",
    "\n",
    "\n",
    "def _email_stats(stats, email):\n",
    "    \"\"\"Send an email...\"\"\"\n",
    "    print(f\"Sending stats to {email}...\")\n",
    "\n",
    "\n",
    "calculate_stats = PythonOperator(\n",
    "    task_id=\"calculate_stats\",\n",
    "    python_callable=_calculate_stats,\n",
    "    templates_dict={\n",
    "        \"input_path\": \"/data/events/{{ds}}.json\",\n",
    "        \"output_path\": \"/data/stats/{{ds}}.csv\",\n",
    "    },\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "fetch_events >> calculate_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7c8aa",
   "metadata": {},
   "source": [
    "## atomic_send DAG\n",
    "- 다수 개의 태스크로 분리하여 원자성 개선\n",
    "- 이메일 발송 기능 별도 태스크로 분리\n",
    "- 이메일 전송에 실패해도 calculate_stats 작업의 결과에 영향 주지 않음\n",
    "- 강한 의존성이 발생할 경우 단일 태스크 내에서 두 작업을 모두 유지하여 하나의 일관된 태스크 단위를 형성하는 것이 더 나을 수 있음(로그인 후 이벤트 API 호출 등)\n",
    "- 대부분의 Airflow 오퍼레이터는 이미 원자성을 유지하도록 설계됨  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f88d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"11_atomic_send\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=dt.datetime(year=2019, month=1, day=1),\n",
    "    end_date=dt.datetime(year=2019, month=1, day=5),\n",
    "    catchup=True,\n",
    ")\n",
    "\n",
    "fetch_events = BashOperator(\n",
    "    task_id=\"fetch_events\",\n",
    "    bash_command=(\n",
    "        \"mkdir -p /data/events && \"\n",
    "        \"curl -o /data/events/{{ds}}.json \"\n",
    "        \"http://events_api:5000/events?\"\n",
    "        \"start_date={{ds}}&\"\n",
    "        \"end_date={{next_ds}}\"\n",
    "    ),\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def _calculate_stats(**context):\n",
    "    \"\"\"Calculates event statistics.\"\"\"\n",
    "    input_path = context[\"templates_dict\"][\"input_path\"]\n",
    "    output_path = context[\"templates_dict\"][\"output_path\"]\n",
    "\n",
    "    events = pd.read_json(input_path)\n",
    "    stats = events.groupby([\"date\", \"user\"]).size().reset_index()\n",
    "\n",
    "    Path(output_path).parent.mkdir(exist_ok=True)\n",
    "    stats.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "calculate_stats = PythonOperator(\n",
    "    task_id=\"calculate_stats\",\n",
    "    python_callable=_calculate_stats,\n",
    "    templates_dict={\n",
    "        \"input_path\": \"/data/events/{{ds}}.json\",\n",
    "        \"output_path\": \"/data/stats/{{ds}}.csv\",\n",
    "    },\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def email_stats(stats, email):\n",
    "    \"\"\"Send an email...\"\"\"\n",
    "    print(f\"Sending stats to {email}...\")\n",
    "\n",
    "\n",
    "def _send_stats(email, **context):\n",
    "    stats = pd.read_csv(context[\"templates_dict\"][\"stats_path\"])\n",
    "    email_stats(stats, email=email)\n",
    "\n",
    "\n",
    "send_stats = PythonOperator(\n",
    "    task_id=\"send_stats\",\n",
    "    python_callable=_send_stats,\n",
    "    op_kwargs={\"email\": \"user@example.com\"},\n",
    "    templates_dict={\"stats_path\": \"/data/stats/{{ds}}.csv\"},\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "fetch_events >> calculate_stats >> send_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abfe9b7",
   "metadata": {},
   "source": [
    "## templated_path DAG (반복)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83825dbf",
   "metadata": {},
   "source": [
    "- 멱등성(dempotency) : 동일한 입력으로 동일한 태스크를 여러 번 호출해도 결과에 효력이 없어야 함\n",
    "- 입력 변경 없이 태스크 다시 실행해도 전체 결과 변경되지 않아야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e287cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"08_templated_path\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=dt.datetime(year=2019, month=1, day=1),\n",
    "    end_date=dt.datetime(year=2019, month=1, day=5),\n",
    ")\n",
    "\n",
    "fetch_events = BashOperator(\n",
    "    task_id=\"fetch_events\",\n",
    "    bash_command=(\n",
    "        \"mkdir -p /data/events && \"\n",
    "        \"curl -o /data/events/{{ds}}.json \" # 템플릿 파일 이름을 설정하여 분할\n",
    "        \"http://events_api:5000/events?\"\n",
    "        \"start_date={{ds}}&\"\n",
    "        \"end_date={{next_ds}}\"\n",
    "    ),\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def _calculate_stats(**context):\n",
    "    \"\"\"Calculates event statistics.\"\"\"\n",
    "    input_path = context[\"templates_dict\"][\"input_path\"]\n",
    "    output_path = context[\"templates_dict\"][\"output_path\"]\n",
    "\n",
    "    events = pd.read_json(input_path)\n",
    "    stats = events.groupby([\"date\", \"user\"]).size().reset_index()\n",
    "\n",
    "    Path(output_path).parent.mkdir(exist_ok=True)\n",
    "    stats.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "calculate_stats = PythonOperator(\n",
    "    task_id=\"calculate_stats\",\n",
    "    python_callable=_calculate_stats,\n",
    "    templates_dict={\n",
    "        \"input_path\": \"/data/events/{{ds}}.json\",\n",
    "        \"output_path\": \"/data/stats/{{ds}}.csv\",\n",
    "    },\n",
    "    # Required in Airflow 1.10 to access templates_dict, deprecated in Airflow 2+.\n",
    "    # provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "fetch_events >> calculate_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98936ef1",
   "metadata": {},
   "source": [
    "# Airflow 콘텍스트 사용하여 태스크 템플릿 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b6298a",
   "metadata": {},
   "source": [
    "- 페이지 뷰 증가는 긍정적임 감성 -> 회사 주식 증가 가능성\n",
    "- 위키미디어 재단 2015년 이후의 모든 페이지 뷰 컴퓨터가 읽을 수 있는 형식ㅇ으로 제공\n",
    "- 페이지 뷰는 gzip 형식으로 다운로드 가능. 시간당 페이지 뷰 수 집계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e6422",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8f602",
   "metadata": {},
   "source": [
    "1. 블로그(검색어)\n",
    "- https://velog.io/@jewon119/01.Flask-%EA%B8%B0%EC%B4%88-Jinja-template(Jinja 템플릿)\n",
    "- https://coding-grandpa.tistory.com/2 (파이썬 보일러플레이트)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
