{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4815e3e7",
   "metadata": {},
   "source": [
    "# 문서 분류 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04574a42",
   "metadata": {},
   "source": [
    "**문서 분류**\n",
    "- 문서가 주어졌을 때 해당 문서의 범주를 분류하는 과제\n",
    "\n",
    "**사용할 데이터**\n",
    "- 네이버 영화 리뷰 말뭉치(NSMC) \n",
    "- 영화 리뷰 문장을 입력으로 하고 해당 문장이 속한 극성의 확률 출력\n",
    "- 확률 값을 적당한 후처리 과정을 거쳐 긍정, 부정처럼 사람이 보기에 좋은 형태로 가공\n",
    "- 감성 분석 : 문장의 극성을 분류하는 과제\n",
    "\n",
    "**모델 구조**\n",
    "- 입력 문장을 토큰화한 뒤 문장 시작과 끝을 알리는 스페셜 토큰 CLS와 SEP를 각각 원래 토큰 시퀀스 앞뒤에 붙임\n",
    "- 이를 BERT 모델에 입력하고 문장 수준의 벡터(pooler_output)을 뽑음\n",
    "- 이 벡터에 작은 추가 모듈을 덧붙여 모델 전체의 출력이 [해당 문장이 긍정일 확률, 해당 문장이 부정일 확률] 형태가 되도록 함\n",
    "\n",
    "**태스크 모듈**\n",
    "- pooler_output 벡터 뒤에 붙은 추가 모듈\n",
    "- 드롭아웃, 가중치 행렬 곱, 소프트맥스 함수 등\n",
    "- 파인튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b56836",
   "metadata": {},
   "source": [
    "# 영화 리뷰 감성 분석 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3ef6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T06:29:14.921781Z",
     "start_time": "2022-08-30T06:29:09.788307Z"
    }
   },
   "outputs": [],
   "source": [
    "# 의존성 패키지 설치 \n",
    "!pip install ratsnlp\n",
    "\n",
    "# 모델 환경 설정 \n",
    "import torch\n",
    "from ratsnlp.nlpbook.classification import ClassificationTrainArguments\n",
    "args = ClassificationTrainArguments(\n",
    "    pretrained_model_name=\"beomi/kcbert-base\", # 프리트레인 마친 언어 모델 이름 (허깅페이스 모델 허브에 등록 필)\n",
    "    downstream_corpus_name=\"nsmc\", # 다운스트림 데이터 이름\n",
    "    # downstream_corpus_root_dir= \"\" # 다운스트림 데이터를 내려받을 위치. 입력하지 않으면 코랩 환경 로컬에 저장 \n",
    "    downstream_model_dir=\"/gdrive/My Drive/nlpbook/checkpoint-doccls\", # 파인튜닝된 모델의 체크포인트가 저장될 위치 \n",
    "    batch_size=32 if torch.cuda.is_available() else 4, # 배치 크기\n",
    "    learning_rate=5e-5, # 러닝 레이트. 1회 스텝에서 모델을 얼마나 업데이트 할지\n",
    "    max_seq_length=128, # 토큰 기준 입력 문장 최대 길이 \n",
    "    epochs=3, # 학습 에포크 수\n",
    "    tpu_cores=0 if torch.cuda.is_available() else 8, # TPU 코어 수\n",
    "    seed=7, # 랜덤 시드 \n",
    ")\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "from ratsnlp import nlpbook\n",
    "nlpbook.set_seed(args)\n",
    "\n",
    "# 로그 출력 로거 설정\n",
    "nlpbook.set_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa83d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 말뭉치 내려 받기 \n",
    "from Korpora import Korpora\n",
    "Korpora.fetch(\n",
    "    corpus_name=args.downstream_corpus_name,\n",
    "    root_dir=args.downstream_corpus_root_dir,\n",
    "    force_download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072a3aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T06:30:56.160502Z",
     "start_time": "2022-08-30T06:30:56.157337Z"
    }
   },
   "outputs": [],
   "source": [
    "# 토크나이저 준비\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    do_lower_case=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd15ea8d",
   "metadata": {},
   "source": [
    "- 딥러닝 모델을 학습하려면 학습 데이터를 배치 단위로 계속 모델에 공급 해야 함\n",
    "- 파이토치에서는 이 역할을 데이터 로더가 수행\n",
    "- 데이터 로더는 데이터셋이 보유하고 있는 인스턴스를 배치 크기만큼 뽑아서 자료형, 데이터 길이 등 정해진 형식에 맞춰 배치를 만들어 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073cc50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "from ratsnlp.nlpbook.classification import NsmcCorpus, ClassificationDataset\n",
    "corpus = NsmcCorpus() # csv 파일 형식의 NSMC 데이터를 문장과 레이블로 읽어 들임 \n",
    "train_dataset = ClassificationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"train\",\n",
    ")\n",
    "\n",
    "# 학습용 데이터 로더 구축. 학습 때 배치 구성은 랜덤으로 하는 것이 좋음 \n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=RandomSampler(train_dataset, replacement=False),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers,\n",
    ")\n",
    "\n",
    "val_dataset = ClassificationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"test\",\n",
    ")\n",
    "\n",
    "# 평가용 데이터 로더 구축 \n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10089c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화 \n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    num_labels=corpus.num_labels,\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "        args.pretrained_model_name,\n",
    "        config=pretrained_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 정의\n",
    "from ratsnlp.nlpbook.classification import ClassificationTask\n",
    "task = ClassificationTask(model, args)\n",
    "\n",
    "# 트레이너 정의\n",
    "trainer = nlpbook.get_trainer(args)\n",
    "\n",
    "# 학습 개시\n",
    "trainer.fit(\n",
    "    task,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a9b74",
   "metadata": {},
   "source": [
    "# 인퍼런스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b05212",
   "metadata": {},
   "source": [
    "**인퍼런스**\n",
    "- 학습을 마친 모델로 실제 과제를 수행하는 행위나 그 과정 \n",
    "- 모델을 문서 분류라는 실전에 투입\n",
    "\n",
    "**웹 서비스**\n",
    "- 네트워크에서 컴퓨터 간에 상호작용을 하기 위해 만들어진 소프트웨어 시스템\n",
    "- 원격 사용자가 보낸 문장을 수신해 해당 문장이 긍정인지 부정인지 응답을 만들고 이 응답을 원격 사용자에게 전달하는 웹 서비스 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6058c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의존성 패키지 설치\n",
    "!pip install ratsnlp\n",
    "\n",
    "# 인퍼런스 설정\n",
    "from ratsnlp.nlpbook.classification import ClassificationDeployArguments\n",
    "args = ClassificationDeployArguments(\n",
    "    pretrained_model_name=\"beomi/kcbert-base\",\n",
    "    downstream_model_dir=\"/gdrive/My Drive/nlpbook/checkpoint-doccls\",\n",
    "    max_seq_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61421800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 로드\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    do_lower_case=False,\n",
    ")\n",
    "\n",
    "# 체크포인트 로드\n",
    "import torch\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "fine_tuned_model_ckpt = torch.load(\n",
    "    args.downstream_model_checkpoint_fpath,\n",
    "    map_location=torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "# BERT 설정 로드 \n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    num_labels=fine_tuned_model_ckpt['state_dict']['model.classifier.bias'].shape.numel(),\n",
    ")\n",
    "model = BertForSequenceClassification(pretrained_model_config) # BERT 모델 초기화 \n",
    "model.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in fine_tuned_model_ckpt['state_dict'].items()}) # 체크포인트 주입\n",
    "model.eval() # 평가 모드 전환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인퍼런스 함수 정의\n",
    "def inference_fn(sentence):\n",
    "    inputs = tokenizer(\n",
    "        [sentence],\n",
    "        max_length=args.max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: torch.tensor(v) for k, v in inputs.items()}) # 파이토치 텐서로 변경 \n",
    "        prob = outputs.logits.softmax(dim=1) # 로짓에 소프트맥스 \n",
    "        positive_prob = round(prob[0][1].item(), 4) # 긍정/부정 확률 소수점 4자리로 반올림\n",
    "        negative_prob = round(prob[0][0].item(), 4) # 긍정/부정 확률 소수점 4자리로 반올림\n",
    "        pred = \"긍정 (positive)\" if torch.argmax(prob) == 1 else \"부정 (negative)\" # 예측 확률의 최댓값 위치에 따라 pred 만들기 \n",
    "    return {\n",
    "        'sentence': sentence,\n",
    "        'prediction': pred,\n",
    "        'positive_data': f\"긍정 {positive_prob}\",\n",
    "        'negative_data': f\"부정 {negative_prob}\",\n",
    "        'positive_width': f\"{positive_prob * 100}%\",\n",
    "        'negative_width': f\"{negative_prob * 100}%\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0fcc80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T06:46:52.483408Z",
     "start_time": "2022-08-30T06:46:52.393400Z"
    }
   },
   "outputs": [],
   "source": [
    "# 웹 서비스 시작\n",
    "# ngrok은 코랩 로컬에서 실행 중인 웹서비스를 안전하게 외부에서 접근 가능하도록 해주는 도구입니다. ngrok을 실행하려면 회원가입 후 로그인을 한 뒤 이곳에 접속해 인증 토큰(authtoken)을 확인해야 합니다. 예를 들어 확인된 authtoken이 test111이라면 다음과 같이 실행합니다.\n",
    "!mkdir /root/.ngrok2 && echo \"authtoken: {이곳에 확인된 인증 토큰을 입력하세요}\" > /root/.ngrok2/ngrok.yml\n",
    "\n",
    "from ratsnlp.nlpbook.classification import get_web_service_app\n",
    "app = get_web_service_app(inference_fn)\n",
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
