{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef43735",
   "metadata": {},
   "source": [
    "# 트랜스포머 주요 요소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d1c11",
   "metadata": {},
   "source": [
    "- 멀티 헤드 어텐션, 피드포워드 뉴럴 네트워크, 잔차 연결 및 레이어 정규화 \n",
    "\n",
    "**피드포워드 뉴럴 네트워크**\n",
    "- 신경망의 한 종류\n",
    "- 입력층, 은닉층, 출력층 3개 계층으로 구성 \n",
    "- 트랜스포머에서 사용하는 피드포워드 뉴럴 네트워크의 활성 함수는 ReLU\n",
    "\n",
    "**잔차 연결**\n",
    "- 블록이나 레이어 계산을 건너뛰는 경로를 하나 둠 \n",
    "- 모델이 다양한 관점에서 블록 계산 수행\n",
    "- 학습을 쉽게 하는 효과 \n",
    "\n",
    "**레이어 정규화**\n",
    "- 미니 배치의 인스턴스 별로 평균을 빼주고 표준편차로 나눠 정규화 수행\n",
    "- 학습 안정 및 속도 빨라지는 등의 효과 있음 \n",
    "- 미니 배치의 인스턴스별로 수행 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f73759",
   "metadata": {},
   "source": [
    "# 트랜스포머 학습 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073662a",
   "metadata": {},
   "source": [
    "**드롭 아웃**\n",
    "- 과적합 현상을 방지하고자 뉴런의 일부를 확률적으로 0으로 대치하여 계산에서 제외 \n",
    "- 입력 임베딩과 최초 블록 사이, 블록과 블록 사이, 마지막 블록과 출력층 사이 등에 적용 \n",
    "- 드롭아웃 비율은 10%로 설정하는 것이 일반적\n",
    "\n",
    "**아담 옵티마이저**\n",
    "- 트랜스포머 모델이 쓰는 최적화 도구\n",
    "- 오차를 줄이는 성능이 좋음 \n",
    "- 핵심 동작 원리는 방향과 보폭을 적절하게 정해줌 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b05b2a",
   "metadata": {},
   "source": [
    "# BERT와 GPT 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c3f735",
   "metadata": {},
   "source": [
    "**GPT**\n",
    "- 언어 모델. 이전 단어들이 주어졌을 때 다음 단어가 무엇인지 맞히는 과정에서 프리트레인 함\n",
    "- 문장 왼쪽부터 오른쪽으로 순차적으로 계산한다는 점에서 일방향\n",
    "\n",
    "**BERT**\n",
    "- 마스크 언어 모델. 문장 중간에 빈칸을 만들고 해당 빈칸에 어떤 단어가 적절할지 맞히는 과정에서 프리트레인 함 \n",
    "- 빈칸 앞뒤 문맥을 모두 살필 수 있다는 점에서 양방향 성격 가짐\n",
    "\n",
    "- GPT는 문장 생성에 BERT는 문장의 의미를 추출하는 데 강점 지님 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561cf890",
   "metadata": {},
   "source": [
    "# 단어/문장 벡터 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6b72e",
   "metadata": {},
   "source": [
    "**파인튜닝**\n",
    "- 프리트레인을 마친 BERT와 그 위의 작은 모듈을 포함한 전체 모델을 문서 분류, 개체명 인식 등 다운스트림 데이터로 업데이트하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859eec9",
   "metadata": {},
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6400c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T05:48:23.434325Z",
     "start_time": "2022-08-29T05:48:18.322971Z"
    }
   },
   "outputs": [],
   "source": [
    "# 토크나이저 선언\n",
    "# kcbert-base 모델이 쓰는 토크나이저 선언 \n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"beomi/kcbert-base\",\n",
    "    do_lower_case=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbc96d1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T05:49:09.906581Z",
     "start_time": "2022-08-29T05:48:25.274967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ab78327f57440b968d6135bf2da9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 300,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.21.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "# BERT 모델이 프리트레인할 때 썼던 토크나이저 그대로 사용 \n",
    "from transformers import BertConfig, BertModel\n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    \"beomi/kcbert-base\"\n",
    ")\n",
    "model = BertModel.from_pretrained(\n",
    "    \"beomi/kcbert-base\",\n",
    "    config=pretrained_model_config,\n",
    ")\n",
    "\n",
    "pretrained_model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d4db2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T05:49:24.185032Z",
     "start_time": "2022-08-29T05:49:24.171505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 19017, 8482, 3, 0, 0, 0, 0, 0, 0], [2, 15830, 5, 3, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT 모델 입력값 \n",
    "sentences = [\"안녕하세요\", \"하이!\"]\n",
    "features = tokenizer(\n",
    "    sentences,\n",
    "    max_length=10,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44b5342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T05:50:17.602356Z",
     "start_time": "2022-08-29T05:50:17.591295Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature 토치 텐서 변환 \n",
    "import torch\n",
    "features = {k: torch.tensor(v) for k, v in features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "badc34d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T05:50:48.323646Z",
     "start_time": "2022-08-29T05:50:48.253709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.6969, -0.8248,  1.7512,  ..., -0.3732,  0.7399,  1.1907],\n",
       "         [-1.4803, -0.4398,  0.9444,  ..., -0.7405, -0.0211,  1.3064],\n",
       "         [-1.4299, -0.5033, -0.2069,  ...,  0.1285, -0.2611,  1.6057],\n",
       "         ...,\n",
       "         [-1.4406,  0.3431,  1.4043,  ..., -0.0565,  0.8450, -0.2170],\n",
       "         [-1.3625, -0.2404,  1.1757,  ...,  0.8876, -0.1054,  0.0734],\n",
       "         [-1.4244,  0.1518,  1.2920,  ...,  0.0245,  0.7572,  0.0080]],\n",
       "\n",
       "        [[ 0.9371, -1.4749,  1.7351,  ..., -0.3426,  0.8050,  0.4031],\n",
       "         [ 1.6095, -1.7269,  2.7936,  ...,  0.3100, -0.4787, -1.2491],\n",
       "         [ 0.4861, -0.4569,  0.5712,  ..., -0.1769,  1.1253, -0.2756],\n",
       "         ...,\n",
       "         [ 1.2362, -0.6181,  2.0906,  ...,  1.3677,  0.8132, -0.2742],\n",
       "         [ 0.5409, -0.9652,  1.6237,  ...,  1.2395,  0.9185,  0.1782],\n",
       "         [ 1.9001, -0.5859,  3.0156,  ...,  1.4967,  0.1924, -0.4448]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.1594,  0.0547,  0.1101,  ...,  0.2684,  0.1596, -0.9828],\n",
       "        [-0.9221,  0.2969, -0.0110,  ...,  0.4291,  0.0311, -0.9955]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT 모델 실행\n",
    "outputs = model(**features)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b520fc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T05:53:13.609263Z",
     "start_time": "2022-08-29T05:53:13.601264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1594,  0.0547,  0.1101,  ...,  0.2684,  0.1596, -0.9828],\n",
       "        [-0.9221,  0.2969, -0.0110,  ...,  0.4291,  0.0311, -0.9955]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 수준 임베딩\n",
    "outputs.last_hidden_state\n",
    "\n",
    "# 문장 수준 임베딩\n",
    "outputs.pooler_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
